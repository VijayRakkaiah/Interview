## ğŸ“ˆ 1. What models did you use and why?

You used 4 regression models:

### ğŸ”¹ Linear Regression
- Fits a straight line to predict fare
- Works well when relationships are linear

### ğŸ”¹ Decision Tree Regressor
- Creates decision rules like:  
  â€œIf distance > 5 km and hour = night, fare = $Xâ€

### ğŸ”¹ Random Forest Regressor
- Combines many trees (a forest) and averages their predictions
- Reduces overfitting and improves accuracy

### ğŸ”¹ XGBoost Regressor
- Builds trees sequentially, each fixing the previous one's errors
- Fast and highly accurate for structured/tabular data

---

## ğŸ” 2. Differences between Linear Regression and Tree-based models?

| Feature           | Linear Regression               | Tree-based Models               |
|------------------|----------------------------------|----------------------------------|
| Relationship     | Assumes straight-line (linear)   | Handles complex, non-linear     |
| Flexibility      | Less flexible                    | More flexible                   |
| Performance      | Lower for complex data           | Higher for structured data      |

---

## ğŸŒ² 3. What is Random Forest?

- A collection of **many decision trees**
- Each tree gives a fare prediction â†’ forest takes the average
- Reduces variance and overfitting

---

## âš¡ 4. What is XGBoost?

- Stands for **Extreme Gradient Boosting**
- Builds trees **one at a time**, each correcting the last
- Fast, powerful, and often state-of-the-art

---

## ğŸ” 5. What is Hyperparameter Tuning?

- Adjusting **settings** of a model to improve performance
- Hyperparameters are not learned â€” theyâ€™re set manually  
  _(e.g., number of trees, tree depth)_
- Used `RandomizedSearchCV` to search for the best combination

---

## ğŸ² 6. What is RandomizedSearchCV?

- Tries **random combinations** of hyperparameters
- More efficient than GridSearchCV (which tries all possible combinations)
- Uses **cross-validation** to find the best settings

---

## ğŸ¤” 7. Why use Random Forest over Linear Regression?

- Linear Regression can't capture complex patterns
- Random Forest:
  - Handles non-linearity
  - Robust to outliers
  - Higher accuracy

---

## ğŸ“‰ 8. How did you evaluate your models?

Used two metrics:
- **MAE (Mean Absolute Error)** â€“ average of absolute differences
- **RMSE (Root Mean Square Error)** â€“ penalizes large errors

---

## ğŸ¥‡ 9. Which model performed best?

- **XGBoost Regressor** had the **lowest error**
- Handled complex patterns and gave most accurate results

---

## âš ï¸ 10. What challenges did you face?

- Dealing with:
  - Invalid coordinates
  - Outliers and missing data
  - Extracting useful features from raw data (e.g., distance from coordinates)

---

## ğŸš« 11. What is overfitting, and how did you prevent it?

- Overfitting = model learns **training data too well**, fails on new data
- Prevented by:
  - Limiting tree depth
  - Cross-validation
  - Using ensemble models like Random Forest & XGBoost

---

## â³ 12. What is cross-validation?

- Splits data into **folds**
- Trains on some folds, validates on others
- Reduces risk of overfitting
- Helps measure **generalization performance**

---

## âš™ï¸ 13. What hyperparameters did you tune?

Examples:

- **Random Forest:**
  - `n_estimators`, `max_depth`, `min_samples_split`

- **XGBoost:**
  - `learning_rate`, `n_estimators`, `max_depth`, `subsample`, `colsample_bytree`

---